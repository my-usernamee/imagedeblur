# -*- coding: utf-8 -*-
"""V2_SAM + CSFF

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/anonymousjpeg/v2-sam-csff.a842c72e-4d80-47da-805a-f1648c458be8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251216/auto/storage/goog4_request%26X-Goog-Date%3D20251216T152538Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da7117414cc980c1772cf72f32e069dddfae347c8eb43410b4c975524c937b575d9f70c34a88eefaa1f7e770acd4db1f0172e3519bb6ef0478f708bf2a4fe572efd6fb23d0cf5f1027d0586dfa2b84f5d031dc811e8ae08a15b9d1ad96abcb76a4e74909cb9410d5d174696daa400c6baa42e2402a752f74eec5c2a8c6fff480cfdd2ae5ec0c5868a4aa05ba7d12bf6f32be6c7885abe63f6b46149a5eedcd666699c8a4c40e1fc519b91fb932f2f9ec782c9ce49daf8758347280d22a17d1b36dab428ab905431dad9cf601663e33346a13378c4e352c6736011112bcaefc2df2c15e1a7fc4e6d3052222e0e5a4ef4a6ed9c4a6f1032d63ff2a5bda42c95afde
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
jishnuparayilshibu_a_curated_list_of_image_deblurring_datasets_path = kagglehub.dataset_download('jishnuparayilshibu/a-curated-list-of-image-deblurring-datasets')

print('Data source import complete.')

!pip install torch torchvision pillow tqdm matplotlib

"""imports

"""

import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as T
from PIL import Image
from tqdm import tqdm
import matplotlib.pyplot as plt

print(f"PyTorch: {torch.__version__}")
print(f"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")

"""datasetup

"""

# For Kaggle - use direct path
path = '/kaggle/input/a-curated-list-of-image-deblurring-datasets'

class GoProDataset(Dataset):
    def __init__(self, root, split='train', img_size=256):
        self.blur_dir = os.path.join(root, 'DBlur', 'Gopro', split, 'blur')
        self.sharp_dir = os.path.join(root, 'DBlur', 'Gopro', split, 'sharp')
        self.img_size = img_size
        self.pairs = []

        for fname in sorted(os.listdir(self.blur_dir)):
            if fname.endswith(('.png', '.jpg', '.jpeg')):
                self.pairs.append((
                    os.path.join(self.blur_dir, fname),
                    os.path.join(self.sharp_dir, fname)
                ))

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        blur_img = Image.open(self.pairs[idx][0]).convert('RGB')
        sharp_img = Image.open(self.pairs[idx][1]).convert('RGB')

        # Store original size
        orig_size = blur_img.size

        # Resize for training
        blur_img = blur_img.resize((self.img_size, self.img_size), Image.BICUBIC)
        sharp_img = sharp_img.resize((self.img_size, self.img_size), Image.BICUBIC)

        # To tensor
        blur_tensor = T.ToTensor()(blur_img)
        sharp_tensor = T.ToTensor()(sharp_img)

        return blur_tensor, sharp_tensor, orig_size

# Create datasets
train_dataset = GoProDataset(path, 'train', img_size=256)
test_dataset = GoProDataset(path, 'test', img_size=256)

print(f"Training samples: {len(train_dataset)}")
print(f"Test samples: {len(test_dataset)}")

model

# Sobel Filter for edge detection
class SobelFilter(nn.Module):
    def __init__(self):
        super().__init__()
        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32)
        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32)
        self.weight_x = nn.Parameter(sobel_x.view(1, 1, 3, 3).repeat(3, 1, 1, 1), requires_grad=False)
        self.weight_y = nn.Parameter(sobel_y.view(1, 1, 3, 3).repeat(3, 1, 1, 1), requires_grad=False)

    def forward(self, x):
        edge_x = F.conv2d(x, self.weight_x, padding=1, groups=3)
        edge_y = F.conv2d(x, self.weight_y, padding=1, groups=3)
        return torch.sqrt(edge_x**2 + edge_y**2 + 1e-6)

# FIXED: Supervised Attention Module (SAM) with Sobel
class SAM(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.sobel = SobelFilter()
        # FIXED: channels + 3 (for RGB edges) instead of channels * 2
        self.conv1 = nn.Conv2d(channels + 3, channels, 3, padding=1)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.conv3 = nn.Conv2d(channels, 3, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, prev_output):
        # Edge detection on previous output (3 channels RGB)
        edges = self.sobel(prev_output)

        # Concatenate features (64 ch) with edges (3 ch) = 67 channels
        x = torch.cat([x, edges], dim=1)
        x = F.relu(self.conv1(x))  # Now expects 67 input channels
        x = F.relu(self.conv2(x))

        # Generate output image
        output = self.sigmoid(self.conv3(x))
        return output

# Encoder Block
class EncoderBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.ReLU(inplace=True)
        )
        self.pool = nn.MaxPool2d(2)

    def forward(self, x):
        feat = self.conv(x)
        return self.pool(feat), feat

# Decoder Block
class DecoderBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)
        self.conv = nn.Sequential(
            nn.Conv2d(out_ch * 2, out_ch, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1),
            nn.ReLU(inplace=True)
        )

    def forward(self, x, skip):
        x = self.up(x)
        x = torch.cat([x, skip], dim=1)
        return self.conv(x)

# Single Stage U-Net
class UNetStage(nn.Module):
    def __init__(self, in_ch=3):
        super().__init__()
        self.enc1 = EncoderBlock(in_ch, 64)
        self.enc2 = EncoderBlock(64, 128)

        self.bottleneck = nn.Sequential(
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(256, 256, 3, padding=1),
            nn.ReLU(inplace=True)
        )

        self.dec1 = DecoderBlock(256, 128)
        self.dec2 = DecoderBlock(128, 64)

        self.out_conv = nn.Conv2d(64, 3, 1)

    def forward(self, x):
        x1, skip1 = self.enc1(x)
        x2, skip2 = self.enc2(x1)

        x3 = self.bottleneck(x2)

        x4 = self.dec1(x3, skip2)
        x5 = self.dec2(x4, skip1)

        return torch.sigmoid(self.out_conv(x5)), x5

# Cross-Stage Feature Fusion (CSFF)
class CSFF(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv = nn.Conv2d(channels * 2, channels, 1)

    def forward(self, x1, x2):
        return F.relu(self.conv(torch.cat([x1, x2], dim=1)))

"""multistage model"""

class MultiStageDeblurNet(nn.Module):
    def __init__(self):
        super().__init__()
        # 3 U-Net stages
        self.stage1 = UNetStage(in_ch=3)
        self.stage2 = UNetStage(in_ch=3)
        self.stage3 = UNetStage(in_ch=3)

        # SAM modules for each stage
        self.sam1 = SAM(64)
        self.sam2 = SAM(64)
        self.sam3 = SAM(64)

        # CSFF between stages
        self.csff1_2 = CSFF(64)
        self.csff2_3 = CSFF(64)

    def forward(self, x):
        # Stage 1
        out1, feat1 = self.stage1(x)
        sam_out1 = self.sam1(feat1, out1)

        # Stage 2 with CSFF
        _, feat2 = self.stage2(out1)
        feat2 = self.csff1_2(feat1, feat2)
        out2, _ = self.stage2(out1)
        sam_out2 = self.sam2(feat2, out2)

        # Stage 3 with CSFF
        _, feat3 = self.stage3(out2)
        feat3 = self.csff2_3(feat2, feat3)
        out3, _ = self.stage3(out2)
        sam_out3 = self.sam3(feat3, out3)

        return [sam_out1, sam_out2, sam_out3]

# Initialize model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = MultiStageDeblurNet().to(device)

print(f"Model parameters: {sum(p.numel() for p in model.parameters()):,}")

"""trainingg setup"""

# DataLoaders
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)

# Loss and Optimizer
criterion = nn.L1Loss()
optimizer = optim.Adam(model.parameters(), lr=2e-4)
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)

# Multi-stage loss weights
stage_weights = [0.5, 0.5, 1.0]

def compute_loss(outputs, target):
    loss = 0
    for i, out in enumerate(outputs):
        loss += stage_weights[i] * criterion(out, target)
    return loss

EPOCHS = 30
best_loss = float('inf')

for epoch in range(EPOCHS):
    model.train()
    train_loss = 0

    for blur, sharp, _ in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
        blur, sharp = blur.to(device), sharp.to(device)

        # Forward
        outputs = model(blur)
        loss = compute_loss(outputs, sharp)

        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

    avg_loss = train_loss / len(train_loader)
    scheduler.step()

    print(f"Epoch {epoch+1}/{EPOCHS} - Loss: {avg_loss:.4f} - LR: {scheduler.get_last_lr()[0]:.6f}")

    # Save best model
    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(model.state_dict(), 'multistage_deblur_best.pth')
        print(f"✓ Best model saved! (Loss: {avg_loss:.4f})")

    # Save checkpoint every 10 epochs
    if (epoch + 1) % 10 == 0:
        torch.save(model.state_dict(), f'checkpoint_epoch_{epoch+1}.pth')

print("Training complete!")

"""visualise"""

def deblur_image(model, image_path, device):
    """Deblur and return same dimensions as input"""
    model.eval()

    # Load image
    img = Image.open(image_path).convert('RGB')
    orig_size = img.size

    # Preprocess
    img_resized = img.resize((256, 256), Image.BICUBIC)
    img_tensor = T.ToTensor()(img_resized).unsqueeze(0).to(device)

    # Predict
    with torch.no_grad():
        outputs = model(img_tensor)
        final_output = outputs[-1]  # Use last stage
        final_output = torch.clamp(final_output, 0, 1)

    # Convert to PIL
    out_img = T.ToPILImage()(final_output[0].cpu())

    # Resize back to original dimensions
    out_img = out_img.resize(orig_size, Image.BICUBIC)

    return out_img

model.eval()

# Test on 3 random samples
test_indices = [5, 50, 100]

fig, axes = plt.subplots(len(test_indices), 3, figsize=(15, 5*len(test_indices)))

for i, idx in enumerate(test_indices):
    blur_path, sharp_path = test_dataset.pairs[idx]

    # Load images
    blur_img = Image.open(blur_path).convert('RGB')
    sharp_img = Image.open(sharp_path).convert('RGB')

    # Deblur
    deblurred = deblur_image(model, blur_path, device)

    # Display
    axes[i, 0].imshow(blur_img)
    axes[i, 0].set_title(f"Blurry\n{blur_img.size}")
    axes[i, 0].axis('off')

    axes[i, 1].imshow(sharp_img)
    axes[i, 1].set_title(f"Ground Truth\n{sharp_img.size}")
    axes[i, 1].axis('off')

    axes[i, 2].imshow(deblurred)
    axes[i, 2].set_title(f"Deblurred\n{deblurred.size}")
    axes[i, 2].axis('off')

plt.tight_layout()
plt.savefig('deblur_results.png', dpi=150, bbox_inches='tight')
plt.show()

print(f"✓ Results saved as 'deblur_results.png'")
print(f"✓ All outputs match original dimensions (1280×720)")